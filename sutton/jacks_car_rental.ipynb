{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4cc16c",
   "metadata": {},
   "source": [
    "## 4.3 Policy Iteration\n",
    "\n",
    "Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited 10 dollars by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are avialable when they are needed, Jack can move them between the two locations overnight, at a cost of 2 dollars per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number if $n$ is $\\frac {\\lambda^n} {n!} e^{-\\lambda}$, where $\\lambda$ is the expected number. Suppose $\\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\\gamma = 0.9$ and formulate this as a continuing finie MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight.\n",
    "\n",
    "### Exercise 4.7:\n",
    "\n",
    "Write a program for policy iteration and re-solve Jac's car rental problem with the following changes. One of Jack's employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs 2.00 dollars, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of 4 dollars must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your rogram, first replicate the results given for the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274fb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da4e66",
   "metadata": {},
   "source": [
    "## The Poisson Distribution\n",
    "\n",
    "First, we will define a pmf for the Poisson distribution. This will be useful for defining other things throughout the problem.\n",
    "\n",
    "$$\n",
    "f(k;\\lambda) = \\frac {\\lambda^ke^{-\\lambda}} {k!}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb6ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def factorial(n):\n",
    "    r = 1\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        r *= i\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def poisson_pmf(lambd, k):\n",
    "    \"\"\"\n",
    "    Compute the poisson probability of k events happening within the time interval lambda.\n",
    "    \"\"\"\n",
    "    return pow(lambd, k) * math.exp(-lambd) / factorial(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80fc49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an event happens 10 or less times is 0.9997262831771443\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "poisson_samples = [poisson_pmf(4.0, i) for i in range(0, 13)]\n",
    "\n",
    "plt.plot(poisson_samples)\n",
    "\n",
    "print(\"The probability that an event happens 10 or less times is %s\" % (np.sum(poisson_samples)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054a034",
   "metadata": {},
   "source": [
    "Note that above, we compute the Poisson Distribution (with $\\lambda = 4$) from k = 0 to 15. The cumalitive probability of an event happening at most 15 times with our given lambda is 99.9%. Because the Poisson distribution is defined up to Infinity, we need a reasonable cut-off point to estimate the probability distributions that we are using. For that reason, we will cut our Poisson calculations at 15. To make sure that probabilities still add up to 1, we will redistribute our distribute by setting a denominator to be the total probability of k <= 15. In addition, because we know that we want the Poisson distributions for $\\lambda$ 2, 3, and 4, we will pre-compute those values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75c97dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9989032810321414, 0.9880954961436427, 0.9488663842071525)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because Poisson is defined up to Infinity, but the vast majority of\n",
    "# probability density is clustered at lower values, we estimate the\n",
    "# distribution by computing up to some max number for our poisson outcome and\n",
    "# dividing by the total density to ensure that all our probabilities add to 1.\n",
    "#\n",
    "# When choosing the poisson cutoff number (defined below), there is a trade-off\n",
    "# between how accurately we our simulating the Poisson distribution and how\n",
    "# efficient our algorithm is.\n",
    "\n",
    "POISSON_COUNT=8\n",
    "\n",
    "poisson_pmf_2 = np.array([poisson_pmf(2.0, i) for i in range(0, POISSON_COUNT)])\n",
    "poisson_pmf_3 = np.array([poisson_pmf(3.0, i) for i in range(0, POISSON_COUNT)])\n",
    "poisson_pmf_4 = np.array([poisson_pmf(4.0, i) for i in range(0, POISSON_COUNT)])\n",
    "\n",
    "original_sum_2 = np.sum(poisson_pmf_2)\n",
    "original_sum_3 = np.sum(poisson_pmf_3)\n",
    "original_sum_4 = np.sum(poisson_pmf_4)\n",
    "\n",
    "poisson_pmf_2 /= np.sum(poisson_pmf_2)\n",
    "poisson_pmf_3 /= np.sum(poisson_pmf_3)\n",
    "poisson_pmf_4 /= np.sum(poisson_pmf_4)\n",
    "\n",
    "# From the PMF values, let us compute the CDF values as well. It is easier to\n",
    "# work with CDF's when calculating probability from a uniform random\n",
    "# number generator.\n",
    "poisson_cdf_2 = np.array(poisson_pmf_2)\n",
    "poisson_cdf_3 = np.array(poisson_pmf_3)\n",
    "poisson_cdf_4 = np.array(poisson_pmf_4)\n",
    "\n",
    "for i in range(1, POISSON_COUNT):\n",
    "    poisson_cdf_2[i] += poisson_cdf_2[i-1]\n",
    "    poisson_cdf_3[i] += poisson_cdf_3[i-1]\n",
    "    poisson_cdf_4[i] += poisson_cdf_4[i-1]\n",
    "\n",
    "\n",
    "original_sum_2, original_sum_3, original_sum_4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6e63cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 7., 5., 0.],\n",
       "       [1., 7., 5., 1.],\n",
       "       [1., 7., 5., 2.],\n",
       "       [1., 7., 5., 3.],\n",
       "       [1., 7., 5., 4.],\n",
       "       [1., 7., 5., 5.],\n",
       "       [1., 7., 5., 6.],\n",
       "       [1., 7., 5., 7.],\n",
       "       [1., 7., 6., 0.],\n",
       "       [1., 7., 6., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will also pre-compute the possible Poisson outcomes.\n",
    "\n",
    "NUM_POISSON_POSSIBILITIES = POISSON_COUNT * POISSON_COUNT * POISSON_COUNT * POISSON_COUNT\n",
    "\n",
    "poisson_outcomes = np.zeros((NUM_POISSON_POSSIBILITIES, 4))\n",
    "poisson_joint_probs = np.ones(NUM_POISSON_POSSIBILITIES)\n",
    "\n",
    "for i in range(POISSON_COUNT):\n",
    "    index_i = i * POISSON_COUNT * POISSON_COUNT * POISSON_COUNT\n",
    "\n",
    "    for j in range(POISSON_COUNT):\n",
    "        index_j = index_i  + (j * POISSON_COUNT * POISSON_COUNT)\n",
    "\n",
    "        for k in range(POISSON_COUNT):\n",
    "            index_k = index_j + (k * POISSON_COUNT)\n",
    "\n",
    "            for l in range (POISSON_COUNT):\n",
    "                index = index_k + l\n",
    "\n",
    "                poisson_outcomes[index, :] = [i, j, k, l]\n",
    "                poisson_joint_probs[index] = poisson_pmf_3[i] * poisson_pmf_2[j] * poisson_pmf_3[k] * poisson_pmf_4[l]   # Rentals B\n",
    "\n",
    "\n",
    "poisson_outcomes[1000:1010, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0267df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: Make sure that the poisson pmf's are summing to 1.\n",
    "\n",
    "DELTA = 1e-5\n",
    "\n",
    "assert(abs(np.sum(poisson_pmf_2) - 1.0) < DELTA)\n",
    "assert(abs(np.sum(poisson_pmf_3) - 1.0) < DELTA)\n",
    "assert(abs(np.sum(poisson_pmf_4) - 1.0) < DELTA)\n",
    "\n",
    "# Sanity Check: Make sure the sum of all join probabilities is 1. \n",
    "\n",
    "assert(abs(np.sum(poisson_joint_probs) - 1.0) < DELTA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0c373c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_poisson(cdf, n):\n",
    "    random_values = np.random.rand(n)\n",
    "    choices = np.zeros(n)\n",
    "    choice_masks = random_values > cdf[0]\n",
    "\n",
    "    for i in range(1, POISSON_COUNT):\n",
    "        choose_this_outcome = choice_masks & (random_values <= cdf[i])\n",
    "        choice_masks &= ~choose_this_outcome\n",
    "        choices += (choose_this_outcome * i)\n",
    "\n",
    "    return choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9274f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: Choosing samples from a Poisson distribution gives\n",
    "# reasonable results.\n",
    "\n",
    "choices = choose_poisson(poisson_cdf_2, 10000)\n",
    "\n",
    "assert np.max(choices) <= POISSON_COUNT - 1\n",
    "assert np.min(choices) >= 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41463976",
   "metadata": {},
   "source": [
    "## Pre-Computing the Rental / Return Possibilities for Each Day\n",
    "\n",
    "During each day, a certain number of cars are rented and returned at each location. Because we are setting a max possibility for our Poisson outcomes, there is a finite number of possible rental / return scenarios. Each rental / return scenario\n",
    "also yields a reward -- revenue from cars returned. We will represent this as a 4-dimensional matrix of possibilities -- the dimensions are (1) returns at location A, (2) returns at location B, (3) rentals at location A, (4) rentals at location B, and the element at the position of the matrix is the probability of that particular outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320fe40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 0.9999999999999999)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poisson_matrix = np.ones((POISSON_COUNT, POISSON_COUNT, POISSON_COUNT, POISSON_COUNT))\n",
    "\n",
    "for i in range(0, POISSON_COUNT):\n",
    "    for j in range(0, POISSON_COUNT):\n",
    "        for k in range(0, POISSON_COUNT):\n",
    "            poisson_matrix[:, i, j, k] *= poisson_pmf_3 # Returns @ A\n",
    "            poisson_matrix[i, :, j, k] *= poisson_pmf_4 # Returns @ B\n",
    "            poisson_matrix[i, j, :, k] *= poisson_pmf_3 # Rentals @ A\n",
    "            poisson_matrix[i, j, k, :] *= poisson_pmf_2 # Rentals @ B\n",
    "\n",
    "np.prod(np.shape(poisson_matrix)), np.sum(poisson_matrix[:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d8a5bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([np.ones((10, 1)), np.zeros((10, 2))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e848f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick sanity check: Need to validate that the sum of all elements in the matrix is 1 (the probability of any possibility happening).\n",
    "DELTA = 1e-5\n",
    "\n",
    "assert(np.abs(np.sum(poisson_matrix[:]) - 1) < DELTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe831c9b",
   "metadata": {},
   "source": [
    "## The Original Problem\n",
    "\n",
    "Let's start by solving the original problem, and verify that we get the same solution described in the book.\n",
    "\n",
    "First, we need to correctly model the problem in terms of the value functions and probability distributions.\n",
    "\n",
    "- The possible states are the number of cars at each location, which is no more than 20. That means a total of 21 * 21 = 441 states.\n",
    "- There are at most 6 possible actions from each state (moving up to 5 cars from one location to another). Note that we ignore strictly dominated actions, such as moving 3 cars from location A to location B and 3 cars from location B to location A, which is strictly worse than not moving any cars at all.\n",
    "- The reward per day is the amount earned from rentals minus the cost of moving cars from the night before.\n",
    "\n",
    "We need to know the full probabilty distribution $p(s', r | s, a)$. So for each state-action pair, we need the probability of each state-reward pair.\n",
    "- There are 441 states, each containing at most 11 actions, so there is an upper limit of 441 * 11 = 4851 state-action pairs.\n",
    "- For each state-action pair, we need the probability of any possible target state, reward pair.\n",
    "- This number will start to get large, so we will define a function that, given a state-action pair, will compute a sequence of probability, next state, reward triples.\n",
    "\n",
    "### Defining Utility Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45539edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important constants we will use throughout the remainder of the notebook.\n",
    "MAX_CARS_PER_LOCATION = 20\n",
    "COST_PER_CAR_MOVED = 2\n",
    "REVENUE_PER_CAR_RENTED = 10\n",
    "MAX_MOVES_DAILY = 5\n",
    "\n",
    "ALL_STATES = np.zeros(((MAX_CARS_PER_LOCATION + 1) * (MAX_CARS_PER_LOCATION + 1), 2)).astype(int)\n",
    "\n",
    "for i in range(0, MAX_CARS_PER_LOCATION + 1):\n",
    "    for j in range(0, MAX_CARS_PER_LOCATION + 1):\n",
    "        ALL_STATES[i * (MAX_CARS_PER_LOCATION + 1) + j, :] = [i, j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d569b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_for_index(indices):\n",
    "    if np.shape(indices) == ():\n",
    "        cars_b = indices % (MAX_CARS_PER_LOCATION + 1)\n",
    "        cars_a = int((indices - cars_b) / (MAX_CARS_PER_LOCATION + 1))\n",
    "        return (cars_a, cars_b)\n",
    "\n",
    "    cars_b = indices % (MAX_CARS_PER_LOCATION + 1)\n",
    "    cars_a = (indices - cars_b) / (MAX_CARS_PER_LOCATION + 1)\n",
    "    return np.concatenate([cars_a, cars_b], axis=-1)\n",
    "\n",
    "\n",
    "def index_for_state(states):\n",
    "    if np.shape(states) == (2,):\n",
    "        return (MAX_CARS_PER_LOCATION + 1) * states[0] + states[1]\n",
    "    \n",
    "    return ((MAX_CARS_PER_LOCATION + 1) * states[:, 0] + states[:, 1]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ef5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make these utilities vectorized compatible.\n",
    "\n",
    "# Actions can take on values between -MAX_MOVES_DAILY and MAX_MOVES_DAILY.\n",
    "\n",
    "ACTION_ARRAY_LEN = 2 * MAX_MOVES_DAILY + 1\n",
    "\n",
    "def index_for_action(action):\n",
    "    assert(abs(action) <= MAX_MOVES_DAILY)\n",
    "    return action + MAX_MOVES_DAILY\n",
    "\n",
    "\n",
    "def action_for_index(index):\n",
    "    assert(index >= 0 and index < ACTION_ARRAY_LEN)\n",
    "    return index - MAX_MOVES_DAILY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7f4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate a set of possible actions given the current state.\n",
    "\n",
    "def possible_actions(state):\n",
    "    cars_a, cars_b = state\n",
    "    min_action = -min(cars_b, MAX_MOVES_DAILY)\n",
    "    max_action = min(cars_a, MAX_MOVES_DAILY)\n",
    "    return range(min_action, max_action + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6946bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's precalculate the set of possible actions that an agent can take at\n",
    "# any given state. We will define this as a 3D matrix, where the dimensions\n",
    "# are [cars_a, cars_b, action_index] and each entry in the matrix is a 1 or 0,\n",
    "# indicating if that particular action can be taken given the number of cars\n",
    "# at locations A and B.\n",
    "possible_actions_matrix = np.zeros((len(ALL_STATES), ACTION_ARRAY_LEN))\n",
    "\n",
    "for i in range(len(ALL_STATES)):\n",
    "    actions = possible_actions(ALL_STATES[i, :])\n",
    "    action_indices = [index_for_action(action) for action in actions]\n",
    "    possible_actions_matrix[i, action_indices] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e636c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: Test a few entries in the possible actions matrix to make sure\n",
    "# they are correct.\n",
    "\n",
    "assert possible_actions_matrix[index_for_state((0, 0)), index_for_action(0)] == 1\n",
    "assert possible_actions_matrix[index_for_state((20, 20)), index_for_action(5)] == 1\n",
    "assert possible_actions_matrix[index_for_state((2, 2)), index_for_action(3)] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c37b93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(state, action):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert(abs(action) <= MAX_MOVES_DAILY)\n",
    "\n",
    "    # Start by moving cars overnight. This is deterministic. We are assuming\n",
    "    # all actions are valid.\n",
    "\n",
    "    cars_a = min(MAX_CARS_PER_LOCATION, state[0] + action)\n",
    "    cars_b = min(MAX_CARS_PER_LOCATION, state[1] - action)\n",
    "\n",
    "    # The following day, a certain number of cars are rented, and a certain\n",
    "    # number are returned to / from each location. We are fully quantifying\n",
    "    # over the entire join distribution of rentals and returns at each\n",
    "    # location.\n",
    "\n",
    "    # NOTE: We are assuming that cars that all the returns on a particular day\n",
    "    # happen before the rentals for that day.\n",
    "\n",
    "    poisson_delta_a = np.reshape(poisson_outcomes[:, 0] - poisson_outcomes[:, 2], (-1, 1))\n",
    "    poisson_delta_b = np.reshape(poisson_outcomes[:, 1] - poisson_outcomes[:, 3], (-1, 1))\n",
    "\n",
    "    # Calculate the next states for each poisson outcome.\n",
    "    next_states = np.concatenate([poisson_delta_a, poisson_delta_b], axis=1)\n",
    "    next_states[:, 0] += cars_a\n",
    "    next_states[:, 1] += cars_b\n",
    "    next_states = np.maximum(0, np.minimum(MAX_CARS_PER_LOCATION, next_states))\n",
    "\n",
    "    # Figure out the total number of rentals for each poisson outcome. We\n",
    "    # need to account for the fact that a location may not have enough cars to\n",
    "    # fulfill all rental requests for the day. A location is paid based on the\n",
    "    # number of fulfilled rentals, not on the number of rental requests.\n",
    "    total_rentals_a = np.minimum(poisson_outcomes[:, 2], poisson_outcomes[:, 0] + cars_a)\n",
    "    total_rentals_b = np.minimum(poisson_outcomes[:, 3], poisson_outcomes[:, 1] + cars_b)\n",
    "\n",
    "    # Figure out the total reward for each Poisson outcome.\n",
    "    rewards = np.zeros(len(next_states))\n",
    "    rewards[:] = -abs(action) * COST_PER_CAR_MOVED\n",
    "    rewards += (total_rentals_a + total_rentals_b) * REVENUE_PER_CAR_RENTED\n",
    "\n",
    "    return next_states, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d045a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: Make sure that the shape of the result of performing an action is correct.\n",
    "\n",
    "next_states, rewards = perform_action((2, 2), 2)\n",
    "\n",
    "assert np.shape(next_states) == (len(poisson_outcomes), 2)\n",
    "assert np.shape(rewards) == (len(poisson_outcomes),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbf0c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_reward(rewards):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return np.sum(poisson_joint_probs * rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94ff1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try performing an action on a demo state and inspect the resulting\n",
    "# possible outcomes.\n",
    "\n",
    "_, rewards1 = perform_action((2, 2), 2)\n",
    "_, rewards2 = perform_action((5, 5), 2)\n",
    "_, rewards3 = perform_action((10, 10), 2)\n",
    "\n",
    "# Sanity check: The expected when there are more cars at both locations should\n",
    "# be more since we have more cars to rent out.\n",
    "\n",
    "exp1 = expected_reward(rewards1)\n",
    "exp2 = expected_reward(rewards2)\n",
    "exp3 = expected_reward(rewards3)\n",
    "\n",
    "assert exp2 > exp1\n",
    "assert exp3 > exp2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de6967",
   "metadata": {},
   "source": [
    "### Defining an Agent / Policy\n",
    "\n",
    "Let's start by defining a basic agent that can:\n",
    "- Observe the current state (# cars at each location)\n",
    "- Pick an action -- or pick probabilities on a set of actions -- to perform at that state (# cars to move between locations)\n",
    "\n",
    "Our initial agent will randomly choose from the set of all possible actions at\n",
    "each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8952adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    An agent that randomly chooses between a set of all possible actions given\n",
    "    the current state.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy(self, states):\n",
    "        \"\"\"\n",
    "        The policy takes the current states and, for each state, returns \n",
    "        the set of probabilities that the agent would take each action. This\n",
    "        is a vectorized operation, so if multiple states are provided, multiple\n",
    "        probability actions are returned.\n",
    "        \"\"\"\n",
    "        state_indices = index_for_state(states)\n",
    "        probs = possible_actions_matrix[state_indices, :]\n",
    "        possible_counts = np.reshape(np.sum(probs, axis=-1), (-1, 1))\n",
    "        probs /= possible_counts\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22921aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: Let's see that our agent is correctly returning policies\n",
    "# where the total probability of choosing an action sums to 1.\n",
    "\n",
    "agent = RandomAgent()\n",
    "probs = agent.policy(ALL_STATES)\n",
    "sum_probs = np.sum(probs, axis=-1)\n",
    "\n",
    "# Asserting that the sum of all probabilities are within 1e-5 from 1.\n",
    "assert np.sum(np.abs(sum_probs - 1) < 1e-5) == len(ALL_STATES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7e136",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "\n",
    "Now that we have a working agent, let's evaluate it. We will use the Bellman Equations to derive the value function for this agent, but also define an evaluation algorithm that will work for evaluating general agents / policies.\n",
    "\n",
    "$$\\large v(s) \\leftarrow \\sum_a \\pi(a | s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v(s')]$$\n",
    "\n",
    "\n",
    "Because each location has at most 20 cars, we have 21 * 21 = 441 possible states that we need values for (including the case of 0 cars at either location). We will define our value function as a 21x21 matrix, where each entry is the value for a particular number of cars.\n",
    "\n",
    "Section 4.1 in the Sutton textbook describes the algorithm for evaluating a policy. We implement the in-place version of the algorithm because it is simpler, often converges faster, and is more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e154784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent_policy(agent, discount, max_k):\n",
    "    \"\"\"\n",
    "    Given an agent, evaluate that agent's policy. This returns the values\n",
    "    for each state given the policy and the number of iterations it took\n",
    "    to converge.\n",
    "    \"\"\" \n",
    "\n",
    "    state_values = np.zeros(len(ALL_STATES))\n",
    "\n",
    "    k = 0\n",
    "    max_delta = float('inf')\n",
    "\n",
    "    while k < max_k:\n",
    "        max_delta = 0\n",
    "\n",
    "        print(f\"Starting iteration {k}\")\n",
    "\n",
    "        action_probs = agent.policy(ALL_STATES)\n",
    "\n",
    "        assert len(action_probs) == len(ALL_STATES)\n",
    "\n",
    "        # PERF: This part of the computation is slow. For each state, computing\n",
    "        # all possible actions from that state by the agent, and for each\n",
    "        # of those actions, collecting the full join poisson distribution.\n",
    "        for i in range(len(ALL_STATES)):\n",
    "            for action_index in range(ACTION_ARRAY_LEN):\n",
    "                action_prob = action_probs[i, action_index]\n",
    "                if action_prob == 0: continue\n",
    "\n",
    "                state = ALL_STATES[i, :]\n",
    "                action = action_for_index(action_index)\n",
    "                next_states, rewards = perform_action(state, action)\n",
    "                next_state_indices = index_for_state(next_states)\n",
    "                next_state_values = state_values[next_state_indices]\n",
    "\n",
    "                new_value = np.sum(poisson_joint_probs * poisson_joint_probs * (rewards + discount * next_state_values))\n",
    "                state_values[i] = new_value\n",
    "\n",
    "        print(f\"Finished iteration {k}\")\n",
    "\n",
    "        k += 1\n",
    "\n",
    "\n",
    "    return np.reshape(state_values, (MAX_CARS_PER_LOCATION + 1, MAX_CARS_PER_LOCATION + 1), order='C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ee009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84, 84, 84, ..., 86, 85, 84])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nex_states, rewards = perform_action((2, 2), 2)\n",
    "index_for_state(next_states).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0ae56ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "Finished iteration 0\n",
      "Starting iteration 1\n",
      "Finished iteration 1\n",
      "Starting iteration 2\n",
      "Finished iteration 2\n",
      "Starting iteration 3\n",
      "Finished iteration 3\n"
     ]
    }
   ],
   "source": [
    "random_agent = RandomAgent()\n",
    "values = evaluate_agent_policy(random_agent, discount=0.9, max_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9d115",
   "metadata": {},
   "source": [
    "## Simulating Agent Runs\n",
    "\n",
    "Let's create a program that takes an agent and a start state, then runs a simulation over *n* days of the agent moving cars and acquiring rewards from rentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d125c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_choose_action(agent, states):\n",
    "    \"\"\"\n",
    "    Use the agent's policy to choose an action at the given state.\n",
    "    \"\"\"\n",
    "    probs = agent.policy(states)\n",
    "\n",
    "    # Choose an action for each state. The choice is a random number between\n",
    "    # 0 and 1 and we will use the policy probabilities to associate these\n",
    "    # random numbers with actions.\n",
    "    prob_choices = np.random.rand(len(states))\n",
    "\n",
    "    action_choices = np.zeros(len(states))\n",
    "\n",
    "    # Using the mask to keep track of which action choices have been locked\n",
    "    # in and which action choices we are still figuring out.\n",
    "    action_choice_masks = prob_choices > probs[:, 0]\n",
    "\n",
    "    for action_index in range(1, ACTION_ARRAY_LEN):\n",
    "        # Accumulating the probabilities. So the probability at the\n",
    "        # action index is now the probability that an action of that index\n",
    "        # or smaller index is chosen.\n",
    "        probs[:, action_index] += probs[:, action_index - 1]\n",
    "\n",
    "        # Of the action choices that have not yet been chosen, figure out\n",
    "        # which ones are associated with the current action index.\n",
    "        choose_current_action = action_choice_masks * (prob_choices <= probs[:, action_index])\n",
    "\n",
    "        action_choices = action_choices + (choose_current_action * action_index)\n",
    "\n",
    "        # Remove the actions that were chosen this pass from the set of\n",
    "        # actions left to choose.\n",
    "        action_choice_masks &= ~choose_current_action\n",
    "\n",
    "    # We have made an action choice for each state.\n",
    "    assert np.sum(action_choice_masks == 0) == len(action_choice_masks)\n",
    "\n",
    "    # Probabilities for agent policy on each state should sum to 1.\n",
    "    assert np.sum(np.abs(probs[:, -1] - 1.0) < 1e-5) == len(action_choice_masks)\n",
    "\n",
    "    return action_choices.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72e33ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: Agent with deterministic policy will result in clear choice.\n",
    "\n",
    "class NeverMoveCarsAgent:\n",
    "    def policy(self, states):\n",
    "        action_index = index_for_action(0)\n",
    "        policy_matrix = np.zeros((len(states), ACTION_ARRAY_LEN))\n",
    "        policy_matrix[:, action_index] = 1.0\n",
    "        return policy_matrix\n",
    "\n",
    "\n",
    "choices1 = agent_choose_action(agent=NeverMoveCarsAgent(), states=ALL_STATES)\n",
    "\n",
    "assert np.sum(choices1 == index_for_action(0)) == len(choices1)\n",
    "\n",
    "# Sanity Check: Random agent correctly chooses actions. \n",
    "\n",
    "choices2 = agent_choose_action(agent=RandomAgent(), states=ALL_STATES)\n",
    "\n",
    "assert np.sum(choices2 < ACTION_ARRAY_LEN) == len(choices2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7542305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(agent, samples, days, discount, initial_state=None):\n",
    "    \"\"\"\n",
    "    Runs a car rental service simulation for the given number of days with the\n",
    "    agent choosing which actions to take.\n",
    "\n",
    "    Can optionally specify an initial state. If the initial state is not\n",
    "    specified, it is randomly generated.\n",
    "\n",
    "    Returns a 2D matrix, where the first dimension in the sample number, and the\n",
    "    second entry is the total reward at the end of day n for the agent.\n",
    "    \"\"\" \n",
    "\n",
    "    rewards = np.zeros((samples, days))\n",
    "\n",
    "    if initial_state is None:\n",
    "        states = np.random.randint(MAX_CARS_PER_LOCATION, size=(samples, 2))\n",
    "    else:\n",
    "        a, b = initial_state\n",
    "        states = np.zeros((samples, 2))\n",
    "        states[:, 0] = a\n",
    "        states[:, 1] = b\n",
    "\n",
    "\n",
    "    for d in range(days):\n",
    "        choices = agent_choose_action(agent, states)\n",
    "        cars_moved = np.abs(choices - index_for_action(0))\n",
    "\n",
    "        today_rewards = -cars_moved * COST_PER_CAR_MOVED\n",
    "        returns_a = choose_poisson(poisson_cdf_3, samples)\n",
    "        returns_b = choose_poisson(poisson_cdf_2, samples)\n",
    "        rentals_a = choose_poisson(poisson_cdf_3, samples)\n",
    "        rentals_b = choose_poisson(poisson_cdf_4, samples)\n",
    "\n",
    "        true_rentals_a = np.minimum(rentals_a, states[:, 0] + returns_a)\n",
    "        true_rentals_b = np.minimum(rentals_b, states[:, 1] + returns_b)\n",
    "\n",
    "        states[:, 0] = np.minimum(states[:, 0] + returns_a - true_rentals_a, MAX_CARS_PER_LOCATION)\n",
    "        states[:, 1] = np.minimum(states[:, 1] + returns_b - true_rentals_b, MAX_CARS_PER_LOCATION)\n",
    "\n",
    "        today_rewards += (true_rentals_a + true_rentals_b) * REVENUE_PER_CAR_RENTED\n",
    "\n",
    "        rewards[:, d] = today_rewards\n",
    "\n",
    "    return rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c703173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE I AM: Just finished implementing the simulation code above. Need to test\n",
    "# it here. Would like various random agents going through multiple days of\n",
    "# car rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdab8c",
   "metadata": {},
   "source": [
    "### Value-Based Agents And Policies\n",
    "\n",
    "Now that we have a way of evaluating an agent, let's define a new agent that follows the state evaluations provided. We will define a new agent whose policy greedily chooses action resulting in the best expected value.\n",
    "\n",
    "When our agent chooses the number of cars to move from one location to another, the outcome state is non-deterministic, since there is some randomness with the number of cars that are returned and returned at each location the next day. For that reason, we need to take the expectation over the non-deterministic outcome to figure out the best action to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8200a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueBasedAgent:\n",
    "    \"\"\"\n",
    "    An agent that greedily chooses the action that results in the highest\n",
    "    expected reward given the state values provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        best_action_index = None\n",
    "        best_action_value = -float('inf')\n",
    "\n",
    "        for action in possible_actions(state):\n",
    "            outcomes = perform_action(state, action)\n",
    "            expectation = expected_reward(outcomes)\n",
    "\n",
    "            if expectation > best_action_value:\n",
    "                best_action_index = index_for_action(action)\n",
    "                best_action_value = expectation\n",
    "\n",
    "        assert(best_action_index is not None)\n",
    "\n",
    "        probs = np.zeros(ACTION_ARRAY_LEN)\n",
    "        probs[best_action_index] = 1.0\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac64d568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10 cars in location A and 2 cars in location B. Our agent chooses to move 1 cars from location B to A.\n",
      "We have 19 cars in location A and 5 cars in location B. Our agent chooses to move 4 cars from location B to A.\n",
      "We have 14 cars in location A and 13 cars in location B. Our agent chooses to move 5 cars from location B to A.\n",
      "We have 17 cars in location A and 18 cars in location B. Our agent chooses to move 5 cars from location B to A.\n",
      "We have 3 cars in location A and 15 cars in location B. Our agent chooses to move 5 cars from location B to A.\n",
      "We have 16 cars in location A and 12 cars in location B. Our agent chooses to move 5 cars from location B to A.\n",
      "We have 15 cars in location A and 0 cars in location B. Our agent chooses to move 1 cars from location A to B.\n",
      "We have 13 cars in location A and 19 cars in location B. Our agent chooses to move 5 cars from location B to A.\n",
      "We have 4 cars in location A and 7 cars in location B. Our agent chooses to move 5 cars from location B to A.\n",
      "We have 4 cars in location A and 4 cars in location B. Our agent chooses to move 3 cars from location B to A.\n"
     ]
    }
   ],
   "source": [
    "NUM_RANDOM_TRIALS = 10\n",
    "\n",
    "trials = np.random.randint(MAX_CARS_PER_LOCATION, size=(NUM_RANDOM_TRIALS, 2))\n",
    "\n",
    "# Creating an agent from the values provided from evaluating the random\n",
    "# policy above.\n",
    "agent = ValueBasedAgent(values)\n",
    "\n",
    "for i in range(NUM_RANDOM_TRIALS):\n",
    "    cars_a, cars_b = trials[i, :]\n",
    "    probs = agent.policy((cars_a, cars_b))\n",
    "    action_index = np.argmax(probs)\n",
    "    action = action_for_index(action_index)\n",
    "\n",
    "    if action < 0:\n",
    "        move_from = \"B\"\n",
    "        move_to = \"A\"\n",
    "    else:\n",
    "        move_from = \"A\"\n",
    "        move_to = \"B\"\n",
    "\n",
    "    print(f\"We have {cars_a} cars in location A and {cars_b} cars in location B. Our agent chooses to move {abs(action)} cars from location {move_from} to {move_to}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1774ed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197.7794912264076"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aaeb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('pokemon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a48e6b4bcafa8268e118fccfda0c48a06d8e2caa41bbf9444eeba1b7211069fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
