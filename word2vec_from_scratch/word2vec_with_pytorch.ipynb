{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with PyTorch\n",
    "\n",
    "**Places to Look:**\n",
    "- [Original Paper](https://arxiv.org/abs/1301.3781)\n",
    "- [Medium Article 1](https://medium.com/@bijil.subhash/code-walkthrough-of-word2vec-pytorch-implementation-3a9ca0ad55a7)\n",
    "- [Medium Article 2](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0)\n",
    "- [Github Repo](https://github.com/OlgaChernytska/word2vec-pytorch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES TO SELF:\n",
    "\n",
    "- Should have a discussion about the difficulty of handling unseen words in\n",
    "  novel situations. Find research papers discussing this.\n",
    "\n",
    "\n",
    "\n",
    "## OUTLINE OF FINAL DOC:\n",
    "\n",
    "- Introduction\n",
    "- Data\n",
    "- Data Preparation\n",
    "- Building the Network\n",
    "  - Architecture & Hyperparameters\n",
    "- Validating the Performance\n",
    "- References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from functools import partial\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If running this locally in an environment that has not yet setup nltk,\n",
    "# need to run this code here.\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34886, 8),\n",
       " Index(['Release Year', 'Title', 'Origin/Ethnicity', 'Director', 'Cast',\n",
       "        'Genre', 'Wiki Page', 'Plot'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 0: How big is this dataset? How big is this corpus?\n",
    "\n",
    "data = pd.read_csv('./wiki_movie_plots_deduped.csv')\n",
    "\n",
    "data.shape, data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Pre-Processor / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.random.rand(len(data)) < 0.8\n",
    "train_data = data[train_mask][\"Plot\"]\n",
    "val_data = data[~train_mask][\"Plot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_N_WORDS = 4\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "def collate_cbow(batch, text_pipeline):\n",
    "    batch_input = []\n",
    "    batch_output = []\n",
    "\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
    "            # Pop the word in the middle of the sequence.\n",
    "            output = token_id_sequence.pop(CBOW_N_WORDS)\n",
    "            input_ = token_id_sequence\n",
    "            batch_input.append(input_)\n",
    "            batch_output.append(output)\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "\n",
    "    return batch_input, batch_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_iter, batch_size):\n",
    "    MIN_WORD_FREQUENCY=1\n",
    "\n",
    "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        map(tokenizer, data_iter),\n",
    "        specials=[\"<unk>\"],\n",
    "        min_freq=MIN_WORD_FREQUENCY\n",
    "    )\n",
    "\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    def text_pipeline(x):\n",
    "        return vocab(tokenizer(x))\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        data_iter,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=partial(collate_cbow, text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "    return dataloader, vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_MAX_NORM = 1\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(CBOWModel, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_norm=EMBED_MAX_NORM\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=20\n",
    "\n",
    "train_dataloader, vocab = build_dataloader(data_iter=train_data.values, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataloader, _ = build_dataloader(data_iter=val_data.values, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code taken from here:\n",
    "# https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/utils/trainer.py\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        epochs,\n",
    "        train_dataloader,\n",
    "        train_steps,\n",
    "        val_dataloader,\n",
    "        val_steps,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        lr_scheduler\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.train_steps = train_steps\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.val_steps = val_steps\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "        self.loss = {\"train\": [], \"val\": []}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self._train_epoch()\n",
    "            self._validate_epoch()\n",
    "\n",
    "            print(\n",
    "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.epochs,\n",
    "                    self.loss[\"train\"][-1],\n",
    "                    self.loss[\"val\"][-1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "    \n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        running_loss = []\n",
    "\n",
    "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
    "            print(\"STARTING BATCH:\", i)\n",
    "            inputs = batch_data[0]\n",
    "            labels = batch_data[1]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            if i == self.train_steps:\n",
    "                break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"train\"].append(epoch_loss)\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(self.val_dataloader, 1):\n",
    "                inputs = batch_data[0]\n",
    "                labels = batch_data[1]\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                if i == self.val_steps:\n",
    "                    break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"val\"].append(epoch_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "STARTING BATCH: 1\n",
      "STARTING BATCH: 2\n",
      "STARTING BATCH: 3\n",
      "STARTING BATCH: 4\n",
      "STARTING BATCH: 5\n",
      "STARTING BATCH: 6\n",
      "STARTING BATCH: 7\n",
      "STARTING BATCH: 8\n",
      "STARTING BATCH: 9\n",
      "STARTING BATCH: 10\n",
      "Epoch: 1/2, Train Loss=11.86478, Val Loss=11.66135\n",
      "Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "STARTING BATCH: 1\n",
      "STARTING BATCH: 2\n",
      "STARTING BATCH: 3\n",
      "STARTING BATCH: 4\n",
      "STARTING BATCH: 5\n",
      "STARTING BATCH: 6\n",
      "STARTING BATCH: 7\n",
      "STARTING BATCH: 8\n",
      "STARTING BATCH: 9\n",
      "STARTING BATCH: 10\n",
      "Epoch: 2/2, Train Loss=11.68298, Val Loss=11.41915\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=2\n",
    "LR=0.01\n",
    "\n",
    "model = CBOWModel(vocab_size=len(vocab), embedding_dim=10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "lr_lambda = lambda epoch: (EPOCHS - epoch) / EPOCHS\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda, verbose=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    epochs=EPOCHS,\n",
    "    train_dataloader=train_dataloader,\n",
    "    train_steps=10,\n",
    "    val_dataloader=val_dataloader,\n",
    "    val_steps=10,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training finished\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cccf65b482c5eebf3e49a7a0a074d637febf6085813a60a937c2a8dd7ab3cc10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
