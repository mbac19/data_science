{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with PyTorch\n",
    "\n",
    "**Places to Look:**\n",
    "- [Original Paper](https://arxiv.org/abs/1301.3781)\n",
    "- [Medium Article 1](https://medium.com/@bijil.subhash/code-walkthrough-of-word2vec-pytorch-implementation-3a9ca0ad55a7)\n",
    "- [Medium Article 2](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES TO SELF:\n",
    "\n",
    "- Should have a discussion about the difficulty of handling unseen words in\n",
    "  novel situations. Find research papers discussing this.\n",
    "\n",
    "\n",
    "\n",
    "## OUTLINE OF FINAL DOC:\n",
    "\n",
    "- Introduction\n",
    "- Data\n",
    "- Data Preparation\n",
    "- Building the Network\n",
    "  - Architecture & Hyperparameters\n",
    "- Validating the Performance\n",
    "- References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from functools import partial\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If running this locally in an environment that has not yet setup nltk,\n",
    "# need to run this code here.\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34886, 8),\n",
       " Index(['Release Year', 'Title', 'Origin/Ethnicity', 'Director', 'Cast',\n",
       "        'Genre', 'Wiki Page', 'Plot'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 0: How big is this dataset? How big is this corpus?\n",
    "\n",
    "data = pd.read_csv('./wiki_movie_plots_deduped.csv')\n",
    "\n",
    "data.shape, data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Pre-Processing / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# Taken from here:\n",
    "# https://www.regular-expressions.info/floatingpoint.html\n",
    "# FLOATING_POINT_REGEX = \"(^[-+]?[0-9]*\\.?[0-9]+$)|(^[-+]?[0-9]*\\.?$)\"\n",
    "# stops = set(stopwords.words('english'))\n",
    "\n",
    "# def preprocess_token(token):\n",
    "#     return token.lower()\n",
    "\n",
    "\n",
    "# def is_valid_token(token):\n",
    "#     return token not in stops and re.match(FLOATING_POINT_REGEX, token) is None\n",
    "\n",
    "# word_tokenize(data['Plot'].iloc[:5])\n",
    "\n",
    "# corpus = set()\n",
    "# tokenized = []\n",
    "\n",
    "# for sentence in data['Plot'].head(100):\n",
    "#     tokenized_sentence = [preprocess_token(token) for token in word_tokenize(sentence.replace(\"-\", \" \")) if is_valid_token(token)]\n",
    "\n",
    "#     corpus.update(tokenized_sentence)\n",
    "#     tokenized.append(tokenized_sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Pre-Processor / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Corpus Size: 40878'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_WORD_FREQUENCY=10\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    map(tokenizer, data[\"Plot\"]),\n",
    "    specials=[\"<unk>\"],\n",
    "    min_freq=MIN_WORD_FREQUENCY\n",
    ")\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "f\"Corpus Size: {len(vocab)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.random.rand(len(data)) < 0.8\n",
    "train_data = data[train_mask][\"Plot\"]\n",
    "val_data = data[~train_mask][\"Plot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(sentence):\n",
    "    return vocab(tokenizer(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# text_pipeline(\"Hello World, my name is flalalala\")\n",
    "\n",
    "# N = 100000\n",
    "# LOG_N = 10\n",
    "# BATCH_SIZE=50\n",
    "\n",
    "# first_n = data[\"Plot\"].head(N)\n",
    "# batch_count = math.ceil(len(first_n) / BATCH_SIZE)\n",
    "\n",
    "# for i in range(batch_count):\n",
    "#     start = i * BATCH_SIZE\n",
    "#     end = start + BATCH_SIZE\n",
    "#     batch = first_n.iloc[start:end]\n",
    "#     input_, output = collate_cbow(batch, text_pipeline)\n",
    "\n",
    "#     if i % LOG_N == 0:\n",
    "#         print(f\"[{i+1}/{batch_count}]: {input_.shape}, {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Corpus Size == 40878'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some statistics on our sentence / tokenized words.\n",
    "\n",
    "# What is the longest plot, in tokens?\n",
    "f\"Corpus Size == {len(vocab)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_N_WORDS = 4\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "def collate_cbow(batch, text_pipeline):\n",
    "    batch_input = []\n",
    "    batch_output = []\n",
    "\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
    "            # Pop the word in the middle of the sequence.\n",
    "            output = token_id_sequence.pop(CBOW_N_WORDS)\n",
    "            input_ = token_id_sequence\n",
    "            batch_input.append(input_)\n",
    "            batch_output.append(output)\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "\n",
    "    return batch_input, batch_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=20\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_cbow, text_pipeline=text_pipeline),\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_cbow, text_pipeline=text_pipeline),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_DIMENSION = 300\n",
    "EMBED_MAX_NORM = 1\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(CBOWModel, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code taken from here:\n",
    "# https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/utils/trainer.py\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        epochs,\n",
    "        train_dataloader,\n",
    "        train_steps,\n",
    "        val_dataloader,\n",
    "        val_steps,\n",
    "        checkpoint_frequency,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        device,\n",
    "        model_dir,\n",
    "        model_name\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.train_steps = train_steps\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.val_steps = val_steps\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.checkpoint_frequency = checkpoint_frequency\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self._train_epoch()\n",
    "            self._validate_epoch()\n",
    "\n",
    "            print(\n",
    "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.epochs,\n",
    "                    self.loss[\"train\"][-1],\n",
    "                    self.loss[\"val\"][-1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "            if self.checkpoint_frequency:\n",
    "                self._save_checkpoint(epoch)\n",
    "\n",
    "    \n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        running_loss = []\n",
    "\n",
    "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
    "            inputs = batch_data[0].to(self.device)\n",
    "            labels = batch_data[1].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            if i == self.train_steps:\n",
    "                break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"train\"].append(epoch_loss)\n",
    "\n",
    "    def _validate_epochs(self):\n",
    "        self.model.eval()\n",
    "        running_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(self.val_dataloader, 1):\n",
    "                inputs = batch_data[0].to(self.device)\n",
    "                labels = batch_data[1].to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                if i == self.val_steps:\n",
    "                    break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"val\"].append(epoch_loss)\n",
    "\n",
    "    \n",
    "    def _save_checkpoint(self, epoch):\n",
    "        epoch_num = epoch + 1\n",
    "\n",
    "        if epoch_num % self.checkpoint_frequency == 0:\n",
    "            model_path = \"checkpoint_{}.pt\".format(str(epoch_num).zfill(3))\n",
    "            modal_path = os.path.join(self.model_dir, model_path)\n",
    "            torch.save(self.model, model_path)\n",
    "\n",
    "    \n",
    "    def save_model(self):\n",
    "        modal_path = os.path.join(self.model_dir, \"model.pt\")\n",
    "        torch.save(self.model, model_path)\n",
    "\n",
    "\n",
    "    def save_loss(self):\n",
    "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
    "        with open(loss_path, \"w\") as fp:\n",
    "            json.dump(self.loss, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=2\n",
    "LR=0.01\n",
    "\n",
    "model = CBOWModel(vocab_size=len(vocab))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# TODO: HERE I AM, BUILDING THE TRAINING PROGRAM!\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cccf65b482c5eebf3e49a7a0a074d637febf6085813a60a937c2a8dd7ab3cc10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
